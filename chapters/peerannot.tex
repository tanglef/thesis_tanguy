\chapter{Reproducible and open library}
\label{chap:peerannot}
\enlargethispage{3\baselineskip}

\begin{keypointstwomargins}{Reproducible and open library}{-2cm}{-1cm}
        \textbf{Key points -- Community based data but what about codes\dots}
        \begin{enumerate}[leftmargin=*]
        \item Reproducibility has been a demand from the scientific community. With crowdsourcing, the coupling of the label gathering step and the aggregation is key to create a classical supervised learning dataset. Different label aggregation strategies can lead to widely different results. Releasing publicly available datasets original version with collected labels would lead to better data quality.
        \item More than the data itself, as the crowdsourcing community is made of researchers with very diverse backgrounds, new models arise quickly. In multiple coding languages (if any) and with personalized data formats. We need formatting propositions that can handle large datasets, are easily accessible and understandable.
        \item Aggregation strategies are often EM-based with a two steps procedure repeated. While performance is an important decision factor in using one strategy over another, how much time they take to run is essential. Especially with large datasets, we find memory scaling issues or a time complexity that forbids usage in applications. 
        \end{enumerate}

        \textbf{Contributions -- peerannot and BenchOpt}
        \begin{enumerate}[leftmargin=*,start=4]
        \item We propose a new Python library \texttt{peerannot} fully documented. An \texttt{identify} module lets users identify ambiguous tasks from datasets using a wide range of strategies. The \texttt{aggregate} module performs label aggregation strategies. The \texttt{aggregate-deep} module uses learning strategies that are deep-learning based and have inserted the aggregation step inside the network's architecture. And the \texttt{train} module allows to train classifiers from paggregated labels. Our library comes with data templates and examples available at \url{http://peerannot.github.io}
        \item We created a crowdsourcing benchmark in the \texttt{BenchOpt} library to easily compare time performance on label aggregation strategies across libraries, on publicly available datasets.
        \end{enumerate}
\end{keypointstwomargins}

\section{\texttt{peerannot}: Open access to different strategies in python}
XXX
\subsection{Identification, aggregation and learning}
XXX
Explain that all previous experiments were run using the library

\subsection{Dataset standardization}
XXX
The json format + ImageFolder to fit pytorch setting 

\section{Benchmarking aggregation strategies with \texttt{Benchopt}}
XXX with Axel
\subsection{What is \texttt{Benchopt}?}
XXX take back the article a bit

\subsection{Results with crowdsourcing real datasets}
XXX Panoplie de datasets let's go sur les aggregations 