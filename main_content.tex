%%% DOCUMENTCLASS
%%%-------------------------------------------------------------------------------

\documentclass[
a4paper, % Stock and paper size.
11pt, % Type size.
% article,
% oneside,
%draft,
twosided,
onecolumn, % Only one column of text on a page.
openright, % Each chapter will start on a recto page.
% openleft, % Each chapter will start on a verso page.
% openany, % A chapter may start on either a recto or verso page.
]{memoir}

\input{header.tex}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\date{2024}
\author{Tanguy Lefort, \\[.5cm] under the supervision of Benjamin Charlier, Alexis Joly and Joseph Salmon.\\[.5cm] }
\title{Label ambiguity in crowdsourcing for classification and expert feedback}


\begin{document}
\includepdf[pages=-]{./main.pdf}  % doit être compilé séparément
\frontmatter

% \maketitle

% \thispagestyle{empty}
% \begin{abstract}
% While classification datasets are composed of more and more data, the need for human expertise to label them is still present. Crowdsourcing platforms are a way to gather expert feedback at a low cost. However, the quality of these labels is not always guaranteed. In this thesis, we focus on the problem of label ambiguity in crowdsourcing. Label ambiguity has mostly two sources: the worker's ability and the task's difficulty. We first present a new indicator, the $\mathrm{WAUM}$ (Weighted Area Under the Magin), to detect ambiguous tasks given to workers. Based on the existing $\mathrm{AUM}$ in the classical supervised setting, this lets us explore large datasets while focusing on tasks that might require more relevant expertise or should be discarded from the actual dataset. We then present a new open-source \texttt{python} library, PeerAnnot, that we developed to handle crowdsourced datasets in image classification. We created a benchmark in the Benchopt library to evaluate our label aggregation strategies for more reproducible results. Finally, we present a case study on the Pl@ntNet dataset, where we evaluate the current state of the platform's label aggregation strategy and propose ways to improve it. This setting with a large number of tasks, experts and classes is highly challenging for current crowdsourcing aggregation strategies. We report consistently better performance against competitors and propose a new aggregation strategy that could be used in the future to improve the quality of the Pl@ntNet dataset. We also release this large dataset of expert feedback that could be used to improve the quality of the current aggregation methods and provide a new benchmark.

% \end{abstract}
\newpage


\input{chapters/abstract.tex}
\input{chapters/glossary.tex}
\input{chapters/remerciements.tex}

\clearpage

\begingroup
\setlength\afterchapskip{2cm}

\tableofcontents*
\endgroup
\clearpage

\mainmatter

\import{chapters}{introduction.tex}
\import{chapters}{waum.tex}
\import{chapters}{peerannot.tex}
\import{chapters}{plantnet.tex}
\import{chapters}{conclusion.tex}

\appendix

\import{chapters}{introduction_french.tex}
\import{chapters}{appendix.tex}
\backmatter

\bibliographystyle{apalike}
%%% BIBLIOGRAPHY
%%% -------------------------------------------------------------

\bibliography{biblio}
\cleardoublepage

\ifthenelse{\isodd{\thepage}}
{\cleardoublepage\mbox{}}
{}

\let\cleardoublepage\clearpage % Add this line to prevent blank pages in the Appendix
\cleardoublepage
\thispagestyle{empty}
\newgeometry{textwidth=.9\paperwidth}
\begin{abstract}
While classification datasets are composed of more and more data, the need for human expertise to label them is still present. Crowdsourcing platforms are a way to gather expert feedback at a low cost. However, the quality of these labels is not always guaranteed. In this thesis, we focus on the problem of label ambiguity in crowdsourcing. Label ambiguity has mostly two sources: the worker's ability and the task's difficulty. We first present a new indicator, the $\mathrm{WAUM}$ (Weighted Area Under the Magin), to detect ambiguous tasks given to workers. Based on the existing $\mathrm{AUM}$ in the classical supervised setting, this lets us explore large datasets while focusing on tasks that might require more relevant expertise or should be discarded from the actual dataset. We then present a new open-source \texttt{python} library, PeerAnnot, that we developed to handle crowdsourced datasets in image classification. We created a benchmark in the Benchopt library to evaluate our label aggregation strategies for more reproducible results. Finally, we present a case study on the Pl@ntNet dataset, where we evaluate the current state of the platform's label aggregation strategy and propose ways to improve it. This setting with a large number of tasks, experts and classes is highly challenging for current crowdsourcing aggregation strategies. We report consistently better performance against competitors and propose a new aggregation strategy that could be used in the future to improve the quality of the Pl@ntNet dataset. We also release this large dataset of expert feedback that could be used to improve the quality of the current aggregation methods and provide a new benchmark.
\end{abstract}

\medskip

\hrule
\medskip
% \begin{center} En français

% \end{center}
\begin{abstract}
    Alors que les jeux de données de classification sont composés d'un nombre croissant de données, le besoin d'expertise humaine pour les étiqueter est toujours présent. Les plateformes de crowdsourcing sont un moyen de recueillir les commentaires d'experts à faible coût. Cependant, la qualité de ces étiquettes n'est pas toujours garantie. Dans cette thèse, nous nous concentrons sur le problème de l'ambiguïté des étiquettes dans le crowdsourcing. L'ambiguïté des étiquettes a principalement deux sources : la capacité du travailleur et la difficulté de la tâche. Nous présentons tout d'abord un nouvel indicateur, le $\mathrm{WAUM}$ (Weighted Area Under the Magin), pour détecter les tâches ambiguës confiées aux travailleurs. Basé sur le $\mathrm{AUM}$ existant dans le cadre supervisé classique, il nous permet d'explorer de grands jeux de données tout en nous concentrant sur les tâches qui pourraient nécessiter une expertise plus pertinente ou qui devraient être éliminées du jeu de données actuel. Nous présentons ensuite une nouvelle bibliothèque \texttt{python} open-source, PeerAnnot, développée pour traiter les jeux de données crowdsourcées dans la classification d'images. Nous avons créé un benchmark dans la bibliothèque Benchopt pour évaluer nos stratégies d'agrégation d'étiquettes afin d'obtenir des résultats reproductibles facilement. Enfin, nous présentons une étude de cas sur l'ensemble de données Pl@ntNet, où nous évaluons l'état actuel de la stratégie d'agrégation d'étiquettes de la plateforme et proposons des moyens de l'améliorer. Ce contexte avec un grand nombre de tâches, d'experts et de classes est très difficile pour les stratégies d'agrégation de crowdsourcing actuelles. Nous faisons état de performances constamment supérieures à celles de nos concurrents et proposons une nouvelle stratégie d'agrégation qui pourrait être utilisée à l'avenir pour améliorer la qualité de l'ensemble de données Pl@ntNet. Nous publions également en plus de ce grand jeu de données, des annotations d'experts qui pourraientt être utilisées pour améliorer la qualité des méthodes d'agrégation actuelles et fournir un nouveau point de référence.
\end{abstract}
\end{document}