%%% DOCUMENTCLASS
%%%-------------------------------------------------------------------------------

\documentclass[
a4paper, % Stock and paper size.
11pt, % Type size.
% article,
% oneside,
%draft,
onecolumn, % Only one column of text on a page.
openright, % Each chapter will start on a recto page.
% openleft, % Each chapter will start on a verso page.
% openany, % A chapter may start on either a recto or verso page.
]{memoir}

\input{header.tex}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\date{2024}
\author{Tanguy Lefort, \\[.5cm] under the supervision of Benjamin Charlier, Alexis Joly and Joseph Salmon.\\[.5cm] }
\title{Label ambiguity in crowdsourcing for classification and expert feedback}


\begin{document}

\frontmatter

\maketitle

\thispagestyle{empty}
\begin{abstract}
While classification datasets are composed of more and more data, the need for human expertise to label them is still present. Crowdsourcing platforms are a way to gather expert feedback at a low cost. However, the quality of these labels is not always guaranteed. In this thesis, we focus on the problem of label ambiguity in crowdsourcing. Label ambiguity has mostly two sources: the worker's ability and the task's difficulty. We first present a new indicator, the $\mathrm{WAUM}$ (Weighted Area Under the Magin), to detect ambiguous tasks given to workers. Based on the existing $\mathrm{AUM}$ in the classical supervised setting, this lets us explore large datasets while focusing on tasks that might require more relevant expertise or should be discarded from the actual dataset. We then present a new open-source Python library, PeerAnnot, that we developed to handle crowdsourced datasets in image classification. We created a benchmark in the Benchopt library to evaluate our label aggregation strategies for more reproducible results. Finally, we present a case study on the Pl@ntNet dataset, where we evaluate the current state of the platform's label aggregation strategy and propose ways to improve it. This setting with a large number of tasks, experts and classes is highly challenging for current crowdsourcing aggregation strategies. We report consistently better performance against competitors and propose a new aggregation strategy that could be used in the future to improve the quality of the Pl@ntNet dataset. We also release this large dataset of expert feedback that could be used to improve the quality of the current aggregation methods and provide a new benchmark.

\end{abstract}
\clearpage


\input{chapters/abstract.tex}
\input{chapters/glossary.tex}
\input{chapters/remerciements.tex}

\clearpage

\begingroup
\setlength\afterchapskip{2cm}

\tableofcontents*
\endgroup
\clearpage

\mainmatter

\import{chapters}{introduction.tex}
\import{chapters}{waum.tex}
\import{chapters}{peerannot.tex}
\import{chapters}{plantnet.tex}
\import{chapters}{conclusion.tex}

\appendix

\import{chapters}{introduction_french.tex}
\import{chapters}{appendix.tex}
\backmatter

\bibliographystyle{apalike}
%%% BIBLIOGRAPHY
%%% -------------------------------------------------------------

\bibliography{biblio}

\end{document}